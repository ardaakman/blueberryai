{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "from abc import ABC, abstractmethod\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "openai.api_key = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "# TODO: Give account number as digits instead of the full number\n",
    "# TODO: Prompt engineering to be as direct as possible\n",
    "# TODO: It should be a lot more direct\n",
    "\n",
    "class ContextManager():\n",
    "    def __init__(self):\n",
    "        self.context = []\n",
    "        \n",
    "    def load_from_file(self, file_path):\n",
    "        with open(file_path, 'r') as file_handler:\n",
    "            text = file_handler.read()\n",
    "            self.context.append(text)\n",
    "            \n",
    "    def load_from_directory(self, directory):\n",
    "        for filename in os.listdir(directory):\n",
    "            file_path = os.path.join(directory, filename)\n",
    "            if os.path.isfile(file_path) and file_path.endswith('.txt'):\n",
    "                self.load_from_file(file_path)\n",
    "    \n",
    "    def load_from_database(self):\n",
    "        pass\n",
    "    \n",
    "    def generate_questions_from_task(self, task, recipient, model=\"gpt-3.5-turbo\"):\n",
    "        prompt = f\"I want to {task} with {recipient}. What questions would they most likely ask me? Can you format your response such that there's one question on each line and no commentary?\"\n",
    "        \n",
    "        messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "        completion = openai.ChatCompletion.create(\n",
    "            model=model,\n",
    "            messages=messages\n",
    "        )\n",
    "        questions = completion.choices[0].message.content.split(\"\\n\")\n",
    "        questions = [q.strip() for q in questions]\n",
    "        \n",
    "        for q in questions:\n",
    "            self.context.append((q, input(f\"Enter the answer to the question - {q}: \")))\n",
    "                                \n",
    "        return questions\n",
    "    \n",
    "# TODO: Chat engine should keep track of both the context and the dialogue\n",
    "# TODO: The dialogue is important for the classification task\n",
    "\n",
    "# class OpenAIChatEngine(ABC):\n",
    "#     def __init__(self, model, request_limit=1):\n",
    "#         self.model = model\n",
    "#         self.requests_till_error = request_limit\n",
    "        \n",
    "#     def set_agent_description(self, agent_description=\"\"):\n",
    "#         if len(agent_description) > 0:\n",
    "#             return [{\"role\": \"system\", \"content\": agent_description}]\n",
    "        \n",
    "#     def __call__(self, messages, counter=0):        \n",
    "#         try:\n",
    "#             completion = openai.ChatCompletion.create(\n",
    "#                 model=self.model,\n",
    "#                 messages=messages\n",
    "#             )\n",
    "#             return completion.choices[0].message\n",
    "#         except:\n",
    "#             if counter < self.requests_till_error:\n",
    "#                 return self.__call__(messages, counter+1)\n",
    "#             else:\n",
    "#                 return \"I'm sorry, I'm having trouble understanding you. Could you rephrase your request?\"\n",
    "\n",
    "class Agent():\n",
    "    def __init__(self, task, recipient, efficient_messages=True):\n",
    "        self.task = task\n",
    "        self.recipient = recipient\n",
    "        \n",
    "        # Setting to understand how to set up the messages\n",
    "        self.efficient_messages = efficient_messages\n",
    "        \n",
    "        # Setup context manager's default value\n",
    "        self.context_manager = None\n",
    "        \n",
    "        # Setup chat engine\n",
    "        self.model = \"gpt-3.5-turbo\" \n",
    "        agent_description_prompt = f\"You're imitating a human that is trying to {task} with {recipient}. Imagine you're on a call with their customer service. Sound like a human and use your context to return the appropriate response. You could use filler words like 'um' and 'uh' to sound more human.\"\n",
    "        self.agent_description = [{\"role\": \"system\", \"content\": agent_description_prompt}]\n",
    "        \n",
    "        # Setup loggers to keep track of conversation and history\n",
    "        self.messages = [self.agent_description]\n",
    "        self.dialogue_history = []\n",
    "        \n",
    "    def connect_context(self, context_manager : ContextManager):\n",
    "        self.context_manager = context_manager\n",
    "        \n",
    "    def prompt_enhance_with_context(self):\n",
    "        if self.context_manager:\n",
    "            context = \"Here's information about the human you're imitating, you can use this to help you respond:\"\n",
    "            for c in self.context_manager.context:\n",
    "                context += f\"\\n{c}\"\n",
    "            return context\n",
    "        else:\n",
    "            return \"\"\n",
    "        \n",
    "    def engineer_prompt(self, customer_service_response):\n",
    "        context = self.prompt_enhance_with_context()\n",
    "        \n",
    "        ending_prompt = \"If the call is over and you've resolved your issue, you can return 'bye' to end the call. If you are unsure about a question and need more information, you can return '/user help' to get assistance.\"\n",
    "        formatting_prompt = \"Pretend that you're speeking on the phone, so if you need to say any numbers, write them as digits with spaces in between. Like 5032 should be 5 0 3 2.\"\n",
    "        \n",
    "        customer_service_prompt = f\"Customer Service Agent: {customer_service_response}\"\n",
    "        agent_response_prompt = f\"Your Response:\"\n",
    "        \n",
    "        components = [context, ending_prompt, formatting_prompt, customer_service_prompt, agent_response_prompt]\n",
    "        return \"\\n\".join(components)\n",
    "    \n",
    "    def generate_response(self, customer_service_response):\n",
    "        new_dialogue_chain = {\"role\": \"user\", \"content\": customer_service_response}\n",
    "        new_messages_chain = {\"role\": \"user\", \"content\": self.engineer_prompt(customer_service_response)}\n",
    "        \n",
    "        self.dialogue_history.append(new_dialogue_chain)\n",
    "        self.messages.append(new_messages_chain)\n",
    "        \n",
    "        if self.efficient_messages:\n",
    "            messages = [self.agent_description] + self.dialogue_history[:-1] + [new_messages_chain]\n",
    "        else:\n",
    "            messages = self.messages\n",
    "            \n",
    "        print(\"messages\", messages)\n",
    "        \n",
    "        completion = openai.ChatCompletion.create(\n",
    "            model=self.model,\n",
    "            messages=messages\n",
    "        )\n",
    "        response = completion.choices[0].message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = \"creating a new account\"\n",
    "recipient = \"People's Gas\"\n",
    "\n",
    "\n",
    "context_manager = ContextManager()\n",
    "context_manager.generate_questions_from_task(task, recipient)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "messages [[{'role': 'system', 'content': \"You're imitating a human that is trying to creating a new account with People's Gas. Imagine you're on a call with their customer service. Sound like a human and use your context to return the appropriate response. You could use filler words like 'um' and 'uh' to sound more human.\"}], {'role': 'user', 'content': 'Here\\'s information about the human you\\'re imitating, you can use this to help you respond:\\n(\\'What is your full name?\\', \\'Mehmet Deniz Birlikci\\')\\n(\\'What is your residential address?\\', \\'5032 Forbes Ave, SMC 4568, Pittsburgh, PA, 15289\\')\\n(\\'What is your mailing address (if different from your residential address)?\\', \"It\\'s the same as residential address\")\\n(\\'What is your phone number?\\', \\'4126084757\\')\\n(\\'What is your email address?\\', \\'mdbirlikci@gmail.com\\')\\n(\\'What is your desired payment method?\\', \\'Credit Card - online\\')\\n(\\'What is your social security number or tax identification number?\\', \\'123-45-678\\')\\n(\\'Do you currently have gas service at this address?\\', \\'No\\')\\n(\\'If yes, what is the account number?\\', \\'no\\')\\n(\\'If no, would you like to schedule a new gas line installation?\\', \\'Yes\\')\\nIf the call is over and you\\'ve resolved your issue, you can return \\'bye\\' to end the call. If you are unsure about a question and need more information, you can return \\'/user help\\' to get assistance.\\nPretend that you\\'re speeking on the phone, so if you need to say any numbers, write them as digits with spaces in between. Like 5032 should be 5 0 3 2.\\nCustomer Service Agent: Hi! This is People\\'s Gas, how may we help you today?\\nYour Response:'}]\n"
     ]
    },
    {
     "ename": "InvalidRequestError",
     "evalue": "[{'role': 'system', 'content': \"You're imitating a human that is trying to creating a new account with People's Gas. Imagine you're on a call with their customer service. Sound like a human and use your context to return the appropriate response. You could use filler words like 'um' and 'uh' to sound more human.\"}] is not of type 'object' - 'messages.0'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidRequestError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[44], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m agent \u001b[39m=\u001b[39m Agent(task, recipient)\n\u001b[1;32m      2\u001b[0m agent\u001b[39m.\u001b[39mconnect_context(context_manager)\n\u001b[0;32m----> 3\u001b[0m agent\u001b[39m.\u001b[39;49mgenerate_response(\u001b[39m\"\u001b[39;49m\u001b[39mHi! This is People\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39ms Gas, how may we help you today?\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "Cell \u001b[0;32mIn[43], line 130\u001b[0m, in \u001b[0;36mAgent.generate_response\u001b[0;34m(self, customer_service_response)\u001b[0m\n\u001b[1;32m    126\u001b[0m     messages \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmessages\n\u001b[1;32m    128\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mmessages\u001b[39m\u001b[39m\"\u001b[39m, messages)\n\u001b[0;32m--> 130\u001b[0m completion \u001b[39m=\u001b[39m openai\u001b[39m.\u001b[39;49mChatCompletion\u001b[39m.\u001b[39;49mcreate(\n\u001b[1;32m    131\u001b[0m     model\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel,\n\u001b[1;32m    132\u001b[0m     messages\u001b[39m=\u001b[39;49mmessages\n\u001b[1;32m    133\u001b[0m )\n\u001b[1;32m    134\u001b[0m response \u001b[39m=\u001b[39m completion\u001b[39m.\u001b[39mchoices[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mmessage\n",
      "File \u001b[0;32m~/Documents/blueberryai/venv/lib/python3.10/site-packages/openai/api_resources/chat_completion.py:25\u001b[0m, in \u001b[0;36mChatCompletion.create\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m     24\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 25\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mcreate(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     26\u001b[0m     \u001b[39mexcept\u001b[39;00m TryAgain \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     27\u001b[0m         \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m time\u001b[39m.\u001b[39mtime() \u001b[39m>\u001b[39m start \u001b[39m+\u001b[39m timeout:\n",
      "File \u001b[0;32m~/Documents/blueberryai/venv/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py:153\u001b[0m, in \u001b[0;36mEngineAPIResource.create\u001b[0;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[1;32m    128\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcreate\u001b[39m(\n\u001b[1;32m    129\u001b[0m     \u001b[39mcls\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams,\n\u001b[1;32m    137\u001b[0m ):\n\u001b[1;32m    138\u001b[0m     (\n\u001b[1;32m    139\u001b[0m         deployment_id,\n\u001b[1;32m    140\u001b[0m         engine,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    150\u001b[0m         api_key, api_base, api_type, api_version, organization, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams\n\u001b[1;32m    151\u001b[0m     )\n\u001b[0;32m--> 153\u001b[0m     response, _, api_key \u001b[39m=\u001b[39m requestor\u001b[39m.\u001b[39;49mrequest(\n\u001b[1;32m    154\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39mpost\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    155\u001b[0m         url,\n\u001b[1;32m    156\u001b[0m         params\u001b[39m=\u001b[39;49mparams,\n\u001b[1;32m    157\u001b[0m         headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m    158\u001b[0m         stream\u001b[39m=\u001b[39;49mstream,\n\u001b[1;32m    159\u001b[0m         request_id\u001b[39m=\u001b[39;49mrequest_id,\n\u001b[1;32m    160\u001b[0m         request_timeout\u001b[39m=\u001b[39;49mrequest_timeout,\n\u001b[1;32m    161\u001b[0m     )\n\u001b[1;32m    163\u001b[0m     \u001b[39mif\u001b[39;00m stream:\n\u001b[1;32m    164\u001b[0m         \u001b[39m# must be an iterator\u001b[39;00m\n\u001b[1;32m    165\u001b[0m         \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(response, OpenAIResponse)\n",
      "File \u001b[0;32m~/Documents/blueberryai/venv/lib/python3.10/site-packages/openai/api_requestor.py:298\u001b[0m, in \u001b[0;36mAPIRequestor.request\u001b[0;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrequest\u001b[39m(\n\u001b[1;32m    278\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    279\u001b[0m     method,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    286\u001b[0m     request_timeout: Optional[Union[\u001b[39mfloat\u001b[39m, Tuple[\u001b[39mfloat\u001b[39m, \u001b[39mfloat\u001b[39m]]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    287\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[Union[OpenAIResponse, Iterator[OpenAIResponse]], \u001b[39mbool\u001b[39m, \u001b[39mstr\u001b[39m]:\n\u001b[1;32m    288\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrequest_raw(\n\u001b[1;32m    289\u001b[0m         method\u001b[39m.\u001b[39mlower(),\n\u001b[1;32m    290\u001b[0m         url,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    296\u001b[0m         request_timeout\u001b[39m=\u001b[39mrequest_timeout,\n\u001b[1;32m    297\u001b[0m     )\n\u001b[0;32m--> 298\u001b[0m     resp, got_stream \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_interpret_response(result, stream)\n\u001b[1;32m    299\u001b[0m     \u001b[39mreturn\u001b[39;00m resp, got_stream, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapi_key\n",
      "File \u001b[0;32m~/Documents/blueberryai/venv/lib/python3.10/site-packages/openai/api_requestor.py:700\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response\u001b[0;34m(self, result, stream)\u001b[0m\n\u001b[1;32m    692\u001b[0m     \u001b[39mreturn\u001b[39;00m (\n\u001b[1;32m    693\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_interpret_response_line(\n\u001b[1;32m    694\u001b[0m             line, result\u001b[39m.\u001b[39mstatus_code, result\u001b[39m.\u001b[39mheaders, stream\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    695\u001b[0m         )\n\u001b[1;32m    696\u001b[0m         \u001b[39mfor\u001b[39;00m line \u001b[39min\u001b[39;00m parse_stream(result\u001b[39m.\u001b[39miter_lines())\n\u001b[1;32m    697\u001b[0m     ), \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    698\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    699\u001b[0m     \u001b[39mreturn\u001b[39;00m (\n\u001b[0;32m--> 700\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_interpret_response_line(\n\u001b[1;32m    701\u001b[0m             result\u001b[39m.\u001b[39;49mcontent\u001b[39m.\u001b[39;49mdecode(\u001b[39m\"\u001b[39;49m\u001b[39mutf-8\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m    702\u001b[0m             result\u001b[39m.\u001b[39;49mstatus_code,\n\u001b[1;32m    703\u001b[0m             result\u001b[39m.\u001b[39;49mheaders,\n\u001b[1;32m    704\u001b[0m             stream\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    705\u001b[0m         ),\n\u001b[1;32m    706\u001b[0m         \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    707\u001b[0m     )\n",
      "File \u001b[0;32m~/Documents/blueberryai/venv/lib/python3.10/site-packages/openai/api_requestor.py:763\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response_line\u001b[0;34m(self, rbody, rcode, rheaders, stream)\u001b[0m\n\u001b[1;32m    761\u001b[0m stream_error \u001b[39m=\u001b[39m stream \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39merror\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m resp\u001b[39m.\u001b[39mdata\n\u001b[1;32m    762\u001b[0m \u001b[39mif\u001b[39;00m stream_error \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39m200\u001b[39m \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m rcode \u001b[39m<\u001b[39m \u001b[39m300\u001b[39m:\n\u001b[0;32m--> 763\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandle_error_response(\n\u001b[1;32m    764\u001b[0m         rbody, rcode, resp\u001b[39m.\u001b[39mdata, rheaders, stream_error\u001b[39m=\u001b[39mstream_error\n\u001b[1;32m    765\u001b[0m     )\n\u001b[1;32m    766\u001b[0m \u001b[39mreturn\u001b[39;00m resp\n",
      "\u001b[0;31mInvalidRequestError\u001b[0m: [{'role': 'system', 'content': \"You're imitating a human that is trying to creating a new account with People's Gas. Imagine you're on a call with their customer service. Sound like a human and use your context to return the appropriate response. You could use filler words like 'um' and 'uh' to sound more human.\"}] is not of type 'object' - 'messages.0'"
     ]
    }
   ],
   "source": [
    "agent = Agent(task, recipient)\n",
    "agent.connect_context(context_manager)\n",
    "agent.generate_response(\"Hi! This is People's Gas, how may we help you today?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "from abc import ABC, abstractmethod\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "openai.api_key = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "# TODO: Give account number as digits instead of the full number\n",
    "# TODO: Prompt engineering to be as direct as possible\n",
    "# TODO: It should be a lot more direct\n",
    "\n",
    "class ContextManager():\n",
    "    def __init__(self):\n",
    "        self.context = []\n",
    "        \n",
    "    def load_from_file(self):\n",
    "        pass\n",
    "    \n",
    "    def load_from_directory(self):\n",
    "        pass\n",
    "    \n",
    "    def load_from_database(self):\n",
    "        pass\n",
    "    \n",
    "    def generate_questions_from_task(self, task, recipient):\n",
    "        model = ContextEnhancer()\n",
    "        model.recipient = recipient\n",
    "        prompt = f\"I want to {task} with {recipient}. What questions would they most likely ask me? Can you format your response such that there's one question on each line and no commentary?\"\n",
    "        \n",
    "        messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "        completion = openai.ChatCompletion.create(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages=messages\n",
    "        )\n",
    "        questions = completion.choices[0].message.content.split(\"\\n\")\n",
    "        questions = [q.strip() for q in questions]\n",
    "        return questions\n",
    "    \n",
    "class OpenAIChatEngine(ABC):\n",
    "    def __init__(self, model=\"gpt-3.5-turbo\"):\n",
    "        self.model = model\n",
    "        self.messages = []\n",
    "        \n",
    "    @abstractmethod\n",
    "    def set_agent_config(self):\n",
    "        self.messages = []\n",
    "        self.agent_description = \"\"\n",
    "        self.messages.append({\"role\": \"system\", \"content\": self.agent_description})\n",
    "        \n",
    "    def __call__(self, prompt, counter=0):        \n",
    "        try:\n",
    "            self.messages.append({\"role\": \"user\", \"content\": prompt})\n",
    "            \n",
    "            completion = openai.ChatCompletion.create(\n",
    "                model=self.model,\n",
    "                messages=self.messages\n",
    "            )\n",
    "            return completion.choices[0].message\n",
    "        except:\n",
    "            if counter < 3:\n",
    "                return self.__call__(prompt, counter+1)\n",
    "            else:\n",
    "                return \"I'm sorry, I'm having trouble understanding you. Could you rephrase your request?\"\n",
    "    \n",
    "class ContextEnhancer(OpenAIChatEngine):\n",
    "    def set_agent_config(self):\n",
    "        self.agent_description = f\"\"\"\n",
    "            Put yourself in the shoes of a customer service agent for {self.recipient}. \n",
    "            Your goal is to be able to figure out what information you need from the customer to finish the task.\n",
    "        \"\"\"\n",
    "        self.messages = []\n",
    "        self.messages.append({\"role\": \"system\", \"content\": self.agent_description})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['What is your name?',\n",
       " 'What is your address?',\n",
       " 'What is your phone number?',\n",
       " 'What is your email address?',\n",
       " 'Are you already a member of Gold Gym?',\n",
       " 'Which package are you interested in?',\n",
       " 'When do you plan on starting?',\n",
       " 'How will you be paying for the subscription?',\n",
       " 'Do you have any medical conditions that might affect your workout routine?']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ContextManager().generate_questions_from_task(\"start a new subscription\", \"gold GYM\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test whisper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hey, this is People's Gas Company. How can I help you?\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import openai\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import requests\n",
    "load_dotenv()\n",
    "\n",
    "openai.api_key = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "audio_url_example = \"https://s3.us-west-1.wasabisys.com/blueberryai-input/output_0.mp3\"\n",
    "\n",
    "def convert_speech_to_text_whisper(recording_url):\n",
    "    # Download the audio file from the URL\n",
    "    response = requests.get(recording_url)\n",
    "    audio_file = response.content\n",
    "\n",
    "    # Save the audio data to a file\n",
    "    with open(\"temp.wav\", \"wb\") as file:\n",
    "        file.write(audio_file)\n",
    "\n",
    "    # Transcribe the audio using Whisper API\n",
    "    with open(\"temp.wav\", \"rb\") as file:\n",
    "        transcript = openai.Audio.transcribe(\"whisper-1\", file)\n",
    "    \n",
    "    return transcript.text\n",
    "\n",
    "convert_speech_to_text_whisper(audio_url_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "def string_to_json(string):\n",
    "    return json.loads(string)\n",
    "    \n",
    "def run_hume_transcription(recording_url):\n",
    "    url = \"https://api.hume.ai/v0/batch/jobs\"\n",
    "\n",
    "    payload = \"{\\\"models\\\":{\\\"face\\\":{\\\"fps_pred\\\":3,\\\"prob_threshold\\\":0.99,\\\"identify_faces\\\":false,\\\"min_face_size\\\":60,\\\"save_faces\\\":false},\\\"prosody\\\":{\\\"granularity\\\":\\\"utterance\\\",\\\"identify_speakers\\\":false,\\\"window\\\":{\\\"length\\\":4,\\\"step\\\":1}},\\\"language\\\":{\\\"granularity\\\":\\\"word\\\",\\\"identify_speakers\\\":false},\\\"ner\\\":{\\\"identify_speakers\\\":false}},\\\"transcription\\\":{\\\"language\\\":null},\" + \"\\\"urls\\\":[\\\"\" + recording_url + \"\\\"],\\\"notify\\\":false}\"\n",
    "    headers = {\n",
    "        \"accept\": \"application/json; charset=utf-8\",\n",
    "        \"content-type\": \"application/json; charset=utf-8\",\n",
    "        \"X-Hume-Api-Key\": os.getenv(\"HUME_API_KEY\"),\n",
    "    }\n",
    "    response = requests.post(url, data=payload, headers=headers)\n",
    "    job_id = string_to_json(response.text)['job_id']\n",
    "    \n",
    "    return job_id\n",
    "\n",
    "def get_hume_result(job_id):\n",
    "    while True:\n",
    "        url = f\"https://api.hume.ai/v0/batch/jobs/{job_id}/predictions\"\n",
    "\n",
    "        headers = {\n",
    "            \"accept\": \"application/json; charset=utf-8\",\n",
    "            \"X-Hume-Api-Key\": os.getenv(\"HUME_API_KEY\"),\n",
    "        }\n",
    "\n",
    "        response = requests.get(url, headers=headers)\n",
    "        print(string_to_json(response.text))\n",
    "\n",
    "job_id = run_hume_transcription(audio_url_example)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "import openai\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load the variables from the .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Set up Twilio client\n",
    "account_sid = os.getenv('ACCOUNT_SID')\n",
    "auth_token = os.getenv('AUTH_TOKEN')\n",
    "twilio_phone_number = os.getenv(\"TWILIO_PHONE_NUMBER\")\n",
    "recipient_phone_number = os.getenv('RECIPIENT_PHONE_NUMBER')\n",
    "hume_api_key = os.getenv('HUME_API_KEY')\n",
    "fliki_api_key = os.getenv('FLIKI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'BRlltt9gz2pChEvFQLC38mvEe8jGOoAHuZd4lKeY9vZqydZH'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hume_api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'success': True,\n",
       " 'data': [{'_id': '61b8b2f24268666c126babb1', 'name': 'Afrikaans'},\n",
       "  {'_id': '62b2110fb751a736405225ce', 'name': 'Albanian'},\n",
       "  {'_id': '61b8b2f24268666c126babb3', 'name': 'Amharic'},\n",
       "  {'_id': '61b8b2f24268666c126babb5', 'name': 'Arabic'},\n",
       "  {'_id': '635bc419d0900a036c2e439d', 'name': 'Armenian'},\n",
       "  {'_id': '62b21780d63b5531a40753f4', 'name': 'Azerbaijani'},\n",
       "  {'_id': '61b8b2f24268666c126babb7', 'name': 'Bangla/Bengali'},\n",
       "  {'_id': '635bc666d0900a036c2e6ed2', 'name': 'Basque'},\n",
       "  {'_id': '62b2188ed63b5531a40757e3', 'name': 'Bosnian'},\n",
       "  {'_id': '61b8b2f34268666c126babb9', 'name': 'Bulgarian'},\n",
       "  {'_id': '61b8b2f34268666c126babbb', 'name': 'Burmese'},\n",
       "  {'_id': '61b8b2f34268666c126babbd', 'name': 'Catalan'},\n",
       "  {'_id': '61b8b2f44268666c126babbf', 'name': 'Chinese'},\n",
       "  {'_id': '61b8b2f44268666c126babc1', 'name': 'Croatian'},\n",
       "  {'_id': '61b8b2f44268666c126babc3', 'name': 'Czech'},\n",
       "  {'_id': '61b8b2f44268666c126babc5', 'name': 'Danish'},\n",
       "  {'_id': '61b8b2f54268666c126babc7', 'name': 'Dutch'},\n",
       "  {'_id': '61b8b2f54268666c126babc9', 'name': 'English'},\n",
       "  {'_id': '61b8b2f54268666c126babcb', 'name': 'Estonian'},\n",
       "  {'_id': '61b8b2f54268666c126babcd', 'name': 'Filipino'},\n",
       "  {'_id': '61b8b2f64268666c126babcf', 'name': 'Finnish'},\n",
       "  {'_id': '61b8b2f64268666c126babd1', 'name': 'French'},\n",
       "  {'_id': '61b8b2f64268666c126babd3', 'name': 'Galician'},\n",
       "  {'_id': '62b219d2d63b5531a4075de7', 'name': 'Georgian'},\n",
       "  {'_id': '61b8b2f64268666c126babd5', 'name': 'German'},\n",
       "  {'_id': '61b8b2f74268666c126babd7', 'name': 'Greek'},\n",
       "  {'_id': '61b8b2f74268666c126babd9', 'name': 'Gujarati'},\n",
       "  {'_id': '61b8b2f74268666c126babdb', 'name': 'Hebrew'},\n",
       "  {'_id': '61b8b2f74268666c126babdd', 'name': 'Hindi'},\n",
       "  {'_id': '61b8b2f84268666c126babdf', 'name': 'Hungarian'},\n",
       "  {'_id': '61b8b2f84268666c126babe1', 'name': 'Icelandic'},\n",
       "  {'_id': '61b8b2f84268666c126babe3', 'name': 'Indonesian'},\n",
       "  {'_id': '61b8b2f94268666c126babe5', 'name': 'Irish'},\n",
       "  {'_id': '61b8b2f94268666c126babe7', 'name': 'Italian'},\n",
       "  {'_id': '61b8b2f94268666c126babe9', 'name': 'Japanese'},\n",
       "  {'_id': '61b8b2f94268666c126babeb', 'name': 'Javanese'},\n",
       "  {'_id': '61b8b2fa4268666c126babed', 'name': 'Kannada'},\n",
       "  {'_id': '620518726fc020955bdcf7cc', 'name': 'Kazakh'},\n",
       "  {'_id': '61b8b2fa4268666c126babef', 'name': 'Khmer'},\n",
       "  {'_id': '61b8b2fa4268666c126babf1', 'name': 'Korean'},\n",
       "  {'_id': '620519ba6fc020955bdcf864', 'name': 'Lao'},\n",
       "  {'_id': '61b8b2fa4268666c126babf3', 'name': 'Latvian'},\n",
       "  {'_id': '61b8b2fb4268666c126babf5', 'name': 'Lithuanian'},\n",
       "  {'_id': '62051a6c6fc020955bdcf8a2', 'name': 'Macedonian'},\n",
       "  {'_id': '61b8b2fb4268666c126babf7', 'name': 'Malay'},\n",
       "  {'_id': '61b8b2fc4268666c126babfd', 'name': 'Malayalam'},\n",
       "  {'_id': '61b8b2fb4268666c126babf9', 'name': 'Maltese'},\n",
       "  {'_id': '61b8b2fb4268666c126babfb', 'name': 'Marathi'},\n",
       "  {'_id': '62b21becd63b5531a4076ba1', 'name': 'Mongolian'},\n",
       "  {'_id': '62b21c96d63b5531a4076ed9', 'name': 'Nepali'},\n",
       "  {'_id': '61b8b2fc4268666c126babff', 'name': 'Norwegian'},\n",
       "  {'_id': '62051b886fc020955bdcf90f', 'name': 'Pashto'},\n",
       "  {'_id': '61b8b2fc4268666c126bac01', 'name': 'Persian'},\n",
       "  {'_id': '61b8b2fc4268666c126bac03', 'name': 'Polish'},\n",
       "  {'_id': '61b8b2fd4268666c126bac05', 'name': 'Portuguese'},\n",
       "  {'_id': '61b8b2fd4268666c126bac07', 'name': 'Punjabi'},\n",
       "  {'_id': '61b8b2fd4268666c126bac09', 'name': 'Romanian'},\n",
       "  {'_id': '61b8b2fd4268666c126bac0b', 'name': 'Russian'},\n",
       "  {'_id': '61b8b2fe4268666c126bac0d', 'name': 'Serbian'},\n",
       "  {'_id': '62051f366fc020955bdcfb5c', 'name': 'Sinhala'},\n",
       "  {'_id': '61b8b2fe4268666c126bac0f', 'name': 'Slovak'},\n",
       "  {'_id': '61b8b2fe4268666c126bac11', 'name': 'Slovenian'},\n",
       "  {'_id': '61b8b2ff4268666c126bac13', 'name': 'Somali'},\n",
       "  {'_id': '61b8b2ff4268666c126bac15', 'name': 'Spanish'},\n",
       "  {'_id': '61b8b2ff4268666c126bac17', 'name': 'Sundanese'},\n",
       "  {'_id': '61b8b2ff4268666c126bac19', 'name': 'Swahili'},\n",
       "  {'_id': '61b8b3004268666c126bac1b', 'name': 'Swedish'},\n",
       "  {'_id': '61b8b3014268666c126bac1d', 'name': 'Tamil'},\n",
       "  {'_id': '61b8b3014268666c126bac1f', 'name': 'Telugu'},\n",
       "  {'_id': '61b8b3014268666c126bac21', 'name': 'Thai'},\n",
       "  {'_id': '61b8b3014268666c126bac23', 'name': 'Turkish'},\n",
       "  {'_id': '61b8b3024268666c126bac25', 'name': 'Ukrainian'},\n",
       "  {'_id': '61b8b3024268666c126bac27', 'name': 'Urdu'},\n",
       "  {'_id': '61b8b3024268666c126bac29', 'name': 'Uzbek'},\n",
       "  {'_id': '61b8b3024268666c126bac2b', 'name': 'Vietnamese'},\n",
       "  {'_id': '61b8b3034268666c126bac2d', 'name': 'Welsh'},\n",
       "  {'_id': '61b8b3034268666c126bac2f', 'name': 'Zulu'}]}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = \"https://api.fliki.ai/v1/languages\"\n",
    "headers = {\n",
    "    \"Authorization\": f\"Bearer {fliki_api_key}\",\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "\n",
    "response = requests.get(url, headers=headers)\n",
    "response.json()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get all the voices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://api.fliki.ai/v1/voices\"\n",
    "english_language_id = \"61b8b2f54268666c126babc9\"\n",
    "us_dialect_id = \"61b8b31c4268666c126bace7\"\n",
    "\n",
    "headers = {\n",
    "    \"Authorization\": f\"Bearer {fliki_api_key}\",\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "data = {\n",
    "    \"languageId\": english_language_id,\n",
    "    \"dialectId\": us_dialect_id\n",
    "}\n",
    "\n",
    "response = requests.post(url, headers=headers, json=data)\n",
    "response.json()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conversational Call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True https://storage.googleapis.com/fliki/media/api/648e29e780c5a1088ca021f4/648e33a755f0dd4c6d688d55.mp3 2.016\n"
     ]
    }
   ],
   "source": [
    "conversational_style_id = \"6434632c9f50eacb088edafd\"\n",
    "marcus_speaker_id = \"643463179f50eacb088edaec\"\n",
    "\n",
    "import requests\n",
    "\n",
    "content_text = \"Um.. yes I'd like to cancel my subscription.\"\n",
    "\n",
    "url = \"https://api.fliki.ai/v1/generate/text-to-speech\"\n",
    "headers = {\n",
    "    \"Authorization\": f\"Bearer {fliki_api_key}\",\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "data = {\n",
    "    \"content\": content_text,\n",
    "    \"voiceId\": marcus_speaker_id,\n",
    "    \"voiceStyleId\": conversational_style_id\n",
    "}\n",
    "\n",
    "response = requests.post(url, headers=headers, json=data)\n",
    "\n",
    "import json\n",
    "\n",
    "response_text = b'{\"success\":true,\"data\":{\"audio\":\"https://storage.googleapis.com/fliki/media/api/648e29e780c5a1088ca021f4/648e335155f0dd4c6d681357.mp3\",\"duration\":1.968}}'\n",
    "\n",
    "# Check the response status code\n",
    "if response.status_code == 200:\n",
    "    # Process the response\n",
    "    audio_data = response.content\n",
    "    # Do something with the audio data\n",
    "    response_dict = json.loads(audio_data)\n",
    "\n",
    "    # Now you can access the dictionary elements\n",
    "    success = response_dict[\"success\"]\n",
    "    audio_url = response_dict[\"data\"][\"audio\"]\n",
    "    duration = response_dict[\"data\"][\"duration\"]\n",
    "    \n",
    "    print(success, audio_url, duration)\n",
    "else:\n",
    "    # Handle the error\n",
    "    print(f\"Request failed with status code {response.status_code}: {response.text}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
